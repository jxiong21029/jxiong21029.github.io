<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta name="description"
        content="An exploration of N-dimensional rotary positional embeddings (RoPE) for vision transformers.">
    <meta property="og:title" content="On N-dimensional Rotary Positional Embeddings">

    <title>N-dimensional Rotary Positional Embeddings</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.11.1/styles/default.min.css">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.11.1/highlight.min.js"></script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script src="https://d3js.org/d3.v7.min.js"></script>
    <style>
        .content {
            width: 100%;
            max-width: 70em;
            margin: 0 auto;
            background-color: white;
            box-sizing: border-box;
            padding: 20px;
        }

        body {
            font-size: 16px;
            line-height: 1.5;
            background-color: #EEEEEE;
        }

        p,
        h1,
        h3,
        h4,
        li,
        th {
            font-family: sans-serif;
        }

        h1 {
            margin-bottom: 0px;
        }

        h2 {
            font-family: Verdana;
            font-size: 22px;
        }

        h4 {
            margin-bottom: 0px;
        }

        .xscroll {
            overflow-x: auto;
        }

        a {
            cursor: pointer
        }

        label {
            font-family: monospace;
            display: flex;
            flex-direction: column;
            font-size: 0.8rem;
            max-width: 150px;
            margin-bottom: 1rem;
        }

        figure {
            display: flex;
            flex-wrap: wrap;
            gap: 1em;
            align-items: start;
            margin: 0;
        }

        .fig-wrapper {
            flex: 1 1 400px;
            max-width: 400px;
        }

        .fig-wrapper-smol {
            flex: 1 1 200px;
            max-width: 200px;
        }

        .figcontrols {
            flex: 1 1 150px;
            max-width: 150px;
        }

        figcaption {
            /* width: 16em; */
            /* flex: 1 0 auto; */
            flex: 1 0 auto;
            width: 300px;
            color: gray;
            font-family: sans-serif;
            font-size: 16px;
        }

        .wide-fig-stuff {
            flex: 1 0 auto;
            width: 400px;
        }

        .wide-figcaption {
            margin: 0;
            margin-bottom: 1rem;
            flex: 1 0 auto;
            color: gray;
            font-family: sans-serif;
            font-size: 16px;
        }

        #rope2d,
        #rope2d_axial {
            display: block;
            image-rendering: pixelated;
            width: 100%;
            aspect-ratio: 1 / 1;
        }

        #rope1d {
            width: 100%
        }

        #rope2d_components {
            display: block;
            image-rendering: pixelated;
            width: 100%;
        }

        .codecaption {
            flex: 1 0 auto;
            width: 30ch;
            color: gray;
            font-family: sans-serif;
            font-size: 16px
        }

        @media (max-width: 600px) {

            .figcontrols,
            figcaption,
            .codecaption,
            .fig-wrapper,
            .wide-fig-stuff {
                flex: 1 1 100%;
            }
        }

        .tooltip {
            position: absolute;
            pointer-events: none;
            background: rgba(255, 255, 255, 0.9);
            border: 1px solid #ccc;
            padding: 2px 6px;
            font-size: 0.75rem;
            border-radius: 4px;
            display: none;
        }

        .grid line {
            stroke: #888;
            stroke-opacity: 0.2;
            shape-rendering: crispEdges;
        }

        .axis text {
            pointer-events: none;
            user-select: none;
        }

        .checkbox_label {
            margin-top: 1rem;
            flex-direction: row;
            align-items: center;
            gap: 0.3rem;
        }

        .aside {
            color: gray;
            margin-left: 42px;
            margin-right: 42px;
        }

        .note {
            color: gray
        }

        .code {
            margin: 0;
            flex: 1 1 100ch;
            font-size: 12px;
            line-height: 16px;
            overflow-x: auto;
        }

        #author {
            margin-top: 0px;
            color: gray;
            font-family: sans-serif;
            font-size: 14px
        }

        th {
            padding-left: 16px;
        }

        td {
            font-family: monospace;
            font-size: 12px;
            padding-left: 16px;
            text-align: right;
        }

        .bigcol {
            font-size: 16px;
            text-align: center;
        }
    </style>
</head>

<body>
    <div id="tooltip" class="tooltip"></div>

    <div class="content">
        <p><a href="/">[back to home]</a></p>

        <h1>On N-dimensional Rotary Positional Embeddings</h1>
        <p id="author">July 26th, 2025 Â· Jerry Xiong
        </p>

        <figure>
            <div class="fig-wrapper">
                <canvas id="rope2d"></canvas>
            </div>
            <div class="fig-wrapper">
                <canvas id="rope2d_axial"></canvas>
            </div>
            <div class="figcontrols">
                <label>resolution
                    <input id="N_range" type="range" min="9" max="99" step="2" value="51" />
                    <input id="N" type="number" min="9" max="99" step="2" value="51" />
                </label>
                <label>min_freq
                    <input id="min_freq_range" type="range" min="0.0" max="100.0" step="0.01" />
                    <input id="min_freq" type="number" min="0.01" max="50.0" step="0.01" value="1" />
                </label>
                <label>max_freq
                    <input id="max_freq_range" type="range" min="0.0" max="100.0" step="0.01" />
                    <input id="max_freq" type="number" min="1.0" max="1000" step="0.1" value="64" />
                </label>
                <label>n_freqs
                    <input id="n_freqs_range" type="range" min="1" max="256" step="1" value="128" />
                    <input id="n_freqs" type="number" min="1" max="256" step="1" value="128" />
                </label>
                <label>direction_spacing
                    <input id="phi_spacing_range" type="range" min="0.0" max="6.28319" step="0.00001" value="1.94161" />
                    <input id="phi_spacing" type="number" min="0.0" max="6.28319" step="0.00001" value="1.94161" />
                    <div style="margin-top: 5px">
                        <button id="btn_pi_2">\(\pi / 2\)</button>
                        <button id="btn_pi_golden">\(\pi / \varphi\)</button>
                        <button id="btn_2pi_golden">\(2\pi / \varphi\)</button>
                    </div>
                </label>
                <!-- <label class="checkbox_label">
                    <input id="axial_directions" type="checkbox" /> axial
            Typical axial RoPE rotates half of each head's components based on
            distance along the x-axis, and the other half based on distance along
            the y-axis, which results in striped artifacts and poorly concentrated
            attention maps. Queries are unable to attend to tokens at specific
            positions without also attending to tokens with similar keys in
            the same row or column. <br><br>
            Instead, by uniformly varying the directions along which distance is
            computed, RoPE can induce attention maps that can be arbitrarily
            concentrated, with scores that decrease smoothly with increasing
            Euclidean distance.
                </label> -->
            </div>

            <figcaption>
                <em>Fig. 1</em>.
                Left: golden gate RoPE, right: axial RoPE. Cosine similarities between a
                fixed query and rotations of that query over varying positions.

            </figcaption>
        </figure>

        <h2>RoPE in one dimension</h2>

        <p>
            One of the simplest ways of encoding relative positional information in
            attention is to add a scalar to each of the attention logits, with a value
            somehow depending on the distance between the corresponding query and key
            (e.g. learned values in
            <a href="https://arxiv.org/abs/1910.10683">T5</a>,
            fixed values decreasing with distance in
            <a href="https://arxiv.org/abs/2108.12409">ALiBi</a>).
        </p>

        <p>
            However, this makes it difficult for a query to attend to any specific (key,
            relative position) pair. In particular, the query must have a component
            pointing in the direction of the desired key, but this increases the
            attention scores with all tokens with that key, <em>regardless</em> of their
            position.
        </p>

        <p>
            The rotary positional embedding (RoPE, <a href="https://arxiv.org/abs/2104.09864">Su et al. 2021</a>)
            is an elegant solution to this problem. Essentially, the query and key
            vectors for every token are rotated by an angle proportional to the token's
            1d coordinate position.
        </p>

        <p class="aside">
            To be specific, each attention head has its \(D\) channel dimensions divided
            into \(D / 2\) dimension pairs. For a given query or key input vector \(x
            \in \mathbb R^D\) located at position \(t\), the \(i\)th dimension pair
            \((x_{2i + 1}, x_{2i + 2})\) is rotated about the origin by an angle
            \(\omega_i \cdot t\), where \(\omega_i\) is the angular frequency
            corresponding to \(i\):
        </p>
        <div class="aside xscroll">
            \[
            \text{RoPE1d} (x, t) =
            {\small
            \begin{pmatrix}
            \cos (\omega_1 t) & -\sin (\omega_1 t) & 0 & 0 & \cdots & 0 & 0 \\
            \sin (\omega_1 t) & \cos (\omega_1 t) & 0 & 0 & \cdots & 0 & 0 \\
            0 & 0 & \cos (\omega_2 t) & -\sin (\omega_2 t) & \cdots & 0 & 0 \\
            0 & 0 & \sin (\omega_2 t) & \cos (\omega_2 t) & \cdots & 0 & 0 \\
            \vdots & \vdots & \vdots & \vdots & \ddots & \vdots & \vdots \\
            0 & 0 & 0 & 0 & \cdots & \cos (\omega_{D / 2} t) & -\sin (\omega_{D / 2} t) \\
            0 & 0 & 0 & 0 & \cdots & \sin (\omega_{D / 2} t) & \cos (\omega_{D / 2} t) \\
            \end{pmatrix}
            }
            x.
            \]
        </div>
        <p class="aside">
            The composition of independent rotations on orthogonal 2d planes is itself
            just a higher-dimensional rotation (see this <a
                href="https://planetmath.org/decompositionoforthogonaloperatorsasrotationsandreflections">article</a>
            or this <a href="https://www.appletonaudio.com/blog/2023/high-dimension-rotation-matrices/">post</a>),
            so RoPE is implementing a single rotation on the vector as a whole.
        </p>
        <p>
            Typically, a range of log-spaced frequencies are selected, where the \(i\)th
            frequency is \[\omega_i = \omega_\text{min} \cdot (\omega_\text{max} /
            \omega_\text{min}) ^{\frac{i}{D/2 - 1}}.\]

            <!-- Intuitively, including a wide range of frequencies enables querying
            positions with varying specificity. For example, a query vector restricted
            to the dimensions with lower frequencies is effectively like using a RoPE
            with a slower rate of rotation, increasing the range of positions targeted
            by the query. -->
        </p>

        <!-- <p>
            RoPE is applied to both the queries and the keys, making it encode only
            relative positional information (as rotating two vectors by the same angle
            doesn't affect their inner product). Attending to a specific (key, relative
            position) pair is possible by setting the query to a large vector in the
            direction of that key, rotated by the angle corresponding to that relative
            position.
        </p> -->

        <p>
            Intuitively, larger values of \(\omega_\text{min}\) and
            \(\omega_\text{max}\) enforce the prior that queries should be more specific
            about position, which can improve expressivity but hurt generalizability.
            Similarly, smaller values of \(\omega_\text{min}\) and \(\omega_\text{max}\)
            lead to more invariance w.r.t position, improving generalizability but
            hurting expressivity.
            <!-- Increasing the distance between \(\omega_\text{min}\)
            and \(\omega_\text{max}\) enables a wide range of specificities but reduces
            the number of query/key dimensions available for the model to use for any
            particular level of specificity. -->
        </p>

        <figure>
            <div class="fig-wrapper">
                <svg id="rope1d" width=400 height=300></svg>
            </div>
            <div class="figcontrols">
                <label>
                    <span>min_freq (\(\omega_\text{min}\))</span>
                    <input type="range" id="minFreq" min="0.0" max="100.0" step="0.1">
                    <input type="number" id="minFreqVal" value="0.1" step="0.0001">
                </label>
                <label>
                    <span>max_freq (\(\omega_\text{max}\))</span>
                    <input type="range" id="maxFreq" min="0.0" max="100.0" step="0.1">
                    <input type="number" id="maxFreqVal" value="100" step="1">

                </label>
                <label>
                    <span>n_freqs (\(D / 2\))</span>
                    <input type="range" id="nFreqs" min="1" max="128" step="1" value="64">
                    <input type="number" id="nFreqsVal" value="64" step="1">
                </label>
            </div>

            <figcaption>
                <em>Fig. 2</em>. Cosine similarities between a fixed query and the
                1d-RoPE-rotated version of itself, over varying positions.
                <!-- <br><br>
                Note that this plot is designed for positions normalized to [-1.0, 1.0].
                In language modeling, positions are usually defined as just the integer
                token index, in which case a commonly used set of parameters is
                approximately equivalent to \(\omega_\text{min}\) = 0.0001 and
                \(\omega_\text{max}\) = 1.0. -->

                <br><br>As more frequencies are added, their periodic oscillations
                cancel each other out, resulting in an attention map concentrated at a
                specific 1d coordinate position.
            </figcaption>
        </figure>

        <h2>Extensions to 2 or more dimensions</h2>

        <p>
            The most common extension of RoPE to 2 dimensions used in vision transformer
            (ViT) implementations today is called axial RoPE, which applies 1d RoPE
            twice: rotating the first \(D/2\) dimensions of the query/key according to
            x-position, and the remaining \(D/2\) dimensions according to y-position.
        </p>

        <p>
            Similar to 1d RoPE, 2d axial RoPE also encodes purely relative positional
            information. However, 2d axial RoPE does not enable attending solely to
            specific (key, relative position) pairs. The first half of a query, which
            rotates according to x-position, contributes the same amount to the
            attention score for a key regardless of the key's y-position. Similarly,
            the second half of a query contributes the same amount regardless of
            x-position. Attending to a token necessarily means attending to any tokens
            with similar keys located in the same row or column, with a cosine
            alignment at least half as large, roughly speaking.
        </p>

        <p>
            The main insight is that, rather than rotating any particular dimension pair
            based on only x-position or based on only y-position, the rotations can
            instead be based on the tokens' positions measured along arbitrary 2d
            directions.
        </p>
        <p class="aside">
            Last year, Heo et al. in their <a href="https://arxiv.org/abs/2403.13298">paper</a> titled "Rotary position
            embedding for vision transformer" described this idea as part of their
            proposed 'Mixed RoPE' method. However, it seems like many of the various
            recent works which cite Heo et al., such as <a href="https://arxiv.org/abs/2408.00714">SAM 2</a>, <a
                href="https://arxiv.org/abs/2412.04431">Infinity</a>, and <a
                href="https://huggingface.co/Qwen/Qwen-Image">Qwen-Image</a>,
            are actually still using axial RoPE, based on their official
            implementations. Maybe this is because it isn't obvious from only reading
            the abstract of Heo et al. that the paper introduces a novel approach
            that differs from axial RoPE.
        </p>

        <p>
            At initialization, unit directions \(\{\mathbf{u}_i\}_{i=1}^{D/2}\),
            \(\mathbf{u}_i \in \mathbb{R}^2\), \(\|\mathbf{u}_i\|_2 = 1\) are selected for
            each of the \(D/2\) dimension pairs. During inference, the angle of rotation
            for the \(i\)th pair is given by the frequency magnitude \(\omega_i\) times
            the dot product of the token's 2d position with \(\mathbf{u}_i\) (whereas
            for 1d RoPE, this angle was just \(\omega_i\) times the token's 1d
            coordinate position).
        </p>

        <p class="aside">
            To be precise, the general N-dimensional RoPE for positions \(\mathbf{t} \in
            \mathbb R^N\) can be written as
        </p>
        <div class="aside xscroll">
            \[
            \text{RoPE} (x, \mathbf{t}) =
            {\small
            \begin{pmatrix}
            \cos (\omega_1 \langle \mathbf{u}_1, \mathbf{t} \rangle) & -\sin (\omega_1
            \langle \mathbf{u}_1, \mathbf{t} \rangle) & \cdots & 0 & 0 \\
            \sin (\omega_1 \langle \mathbf{u}_1, \mathbf{t} \rangle) & \cos (\omega_1
            \langle \mathbf{u}_1, \mathbf{t} \rangle) & \cdots & 0 & 0 \\
            \vdots & \vdots & \ddots & \vdots & \vdots \\
            0 & 0 & \cdots & \cos (\omega_{D / 2} \langle \mathbf{u}_{D / 2}, \mathbf{t}
            \rangle) & -\sin (\omega_{D / 2} \langle \mathbf{u}_{D / 2}, \mathbf{t}
            \rangle) \\
            0 & 0 & \cdots & \sin (\omega_{D / 2} \langle \mathbf{u}_{D / 2}, \mathbf{t}
            \rangle) & \cos (\omega_{D / 2} \langle \mathbf{u}_{D / 2}, \mathbf{t}
            \rangle) \\
            \end{pmatrix}
            }
            x.
            \]
        </div>
        <p class="aside">
            This includes axial RoPE as a special case where each \(\mathbf{u}_i\) is in
            the standard basis.
        </p>

        <p>
            In other words, the \(i\)th dimension pair is rotated by an angle
            proportional to "the position of the token measured according to the
            direction \(\mathbf{u}_i\)". By selecting \(\{\mathbf{u}_i\}_{i=1}^{D /
            2}\) uniformly from the unit circle rather than constraining them to
            axis-aligned directions, it turns out that RoPE can also produce
            concentrated attention maps in 2d!
        </p>


        <figure>
            <div class="fig-wrapper-smol">
                <canvas id="rope2d_components"></canvas>
            </div>
            <div class="wide-fig-stuff">
                <p class="wide-figcaption">
                    <em>Fig. 3</em>. Cosine similarities between a fixed query and the
                    2d-RoPE-rotated version of itself over varying positions. Here, the
                    similarities are evaluated on a prefix of the dimension pairs. The
                    attention map becomes gradually more concentrated as the number of
                    frequencies increases.
                </p>

                <label># frequencies
                    <input id="n_components_range" type="range" min="1" max="256" step="1" value="128" />
                    <input id="n_components" type="number" min="1" max="256" step="1" value="128" />
                </label>
            </div>
        </figure>

        <p>
            NOTE: since the sequence lengths relevant for language modeling are
            typically much longer than the side length of image inputs for vision
            transformers, the minimum and maximum frequency magnitudes should be
            adjusted to compensate. I recommend using a coordinate system with positions
            normalized to [-1.0, 1.0] and using \(\omega_\text{min}\) between 0.2 and
            1.0 and \(\omega_\text{max}\) between 20 and 100.
        </p>

        <h2>Selecting frequency directions</h2>
        <p>
            Mixed RoPE (<a href="https://arxiv.org/abs/2403.13298">Heo et al. 2024</a>)
            initializes \(\{\mathbf{u}_i\}_{i=1}^{D / 2}\) by sampling uniformly random
            vectors from the unit circle, then treating the frequency vectors
            \(\mathbf{f}_i = \omega_i \mathbf{u}_i\) as learnable parameters. However,
            it's unclear a priori whether learnable frequency vectors are actually
            beneficial, especially if the frequencies are selected in such a way that
            it's already possible to query unique positions at initialization.
        </p>
        <p class="aside">
            Intuitively, typical neural network parameters, e.g. the weights of a linear
            layer, have the property that a gradient step on some training input will
            generally only induce a large change in the layer's outputs for inputs
            similar to that training input.

            In contrast, changes to the frequency vectors of RoPE will nontrivially modify the
            attention scores between almost all query-key pairs, which could make these
            frequencies less amenable to gradient optimization.
        </p>

        <p>
            If these frequency vectors are kept frozen instead, then ideally we would
            want to initialize them in a more principled, deterministic fashion. In the
            initial discussion on the EleutherAI discord,
            <a href="https://github.com/ad8e">Kevin Yin</a> suggested arranging the
            frequency vectors for each head in order of increasing magnitude, and
            rotating the \(i\)th vector to an angle of \(i\cdot 2\pi / \varphi\) where
            \(\varphi = (1 + \sqrt{5}) / 2\) is the golden ratio.
            This is the approach used in the experiments below, though, as Kevin
            later pointed out, rotating by \(i \cdot \pi / \varphi\) is likely to
            perform better.
            <span style="color: gray">
                (edit: For a proper explanation about <em>why</em> this works well,
                check out Kevin's <a href="https://research.novelai.net/rope/">blog
                    post</a>.)
            </span>

            <!-- As long as \(\Delta\phi\) isn't too small or
            <a href="https://en.wikipedia.org/wiki/Diophantine_approximation">well
            approximated</a> as a rational fraction of \(2\pi\), the resulting vectors
            will be distributed uniformly. -->
            <!-- A good option is \(\Delta\phi = 2\pi / \varphi\) where \(\varphi = (1 +
            \sqrt{5}) / 2\) is the golden ratio. -->
        </p>

        <p>
            For embedding positions in more than 2 dimensions, one option that is easy
            to implement and works well enough is to take samples from \(U(0, 1)\)
            <a
                href="https://extremelearning.com.au/unreasonable-effectiveness-of-quasirandom-sequences/">quasi-randomly</a>
            via the generalized golden ratio, mapping them to Gaussian samples using the
            inverse CDF, and then normalizing to length one. An example implementation
            is provided <a href="#ropend_impl">here</a>.
            <span style="color: gray">
                (edit: nor wrote a great <a href="https://nor-blog.pages.dev/posts/2025-07-28-deriving-rope/">blog
                    post</a> discussing better ways to select frequency directions in >2
                dimensions.)
            </span>
        </p>

        <h2>ViT experiments</h2>

        <p>
            Here, I compare:
        </p>

        <ul>
            <li>
                Learned absolute positional embedding (APE, e.g. <a href="https://arxiv.org/abs/2010.11929">Dosovitskiy
                    et al. 2020</a>)
            </li>
            <li>
                Fixed sinusoidal positional embedding (SinCos)
                <ul>
                    <li class="note">
                        an absolute positional embedding approach without trainable
                        parameters
                        (<a href="https://arxiv.org/abs/2104.02057">Chen et al. 2021</a>,
                        <a href="https://arxiv.org/abs/2205.01580">Beyer et al. 2022</a>)
                    </li>
                    <li class="note">
                        similar to the positional embedding from the original
                        transformer paper
                        (<a href="https://arxiv.org/abs/1706.03762">Vaswani et al. 2017</a>),
                        but with one half derived from x-positions and the other half
                        from y.
                    </li>
                </ul>
            </li>
            <li>
                Axial RoPE
            </li>
            <li>
                RoPE with learned, randomly initialized frequency vectors (mixed RoPE, <a
                    href="https://arxiv.org/abs/2403.13298">Heo et al. 2024</a>)
            </li>
            <li>
                LieRE (<a href="https://arxiv.org/abs/2406.10322">Ostmeier et al. 2024</a>)
                <ul>
                    <li class="note">a rotary positional embedding scheme
                        where the rotation is parameterized by the matrix exponential of
                        the weighted sum of learnable skew-symmetric matrices, with
                        weights equal to token position</li>
                </ul>
            </li>
            <li>
                RoPE with fixed directions rotated according to the golden ratio (golden gate RoPE)
                <sup id="footnote_1_ref"><a href="#footnote_1">[1]</a></sup>
            </li>
        </ul>

        <h3>CIFAR10</h3>

        <p>
            I trained some small (7M parameter) ViTs with 4x4 patches for 200 epochs on
            CIFAR10. I searched over \(\omega_\text{min} \in \{0.5, 1.0\}\) with
            \(\omega_\text{max} = 100 \cdot \omega_\text{min}\). Below, I'm reporting
            the best validation negative log-likelihood (NLL) and accuracy, mean Â± std
            over 2 seeds, for each approach. See hyperparameters <a href="#cifar10_hyperparams">here</a> and
            code <a
                href="https://github.com/jxiong21029/archibox/blob/main/archibox/ropend/cifar_classification.py">here</a>.

        </p>

        <p>
            The official implementation of mixed RoPE uses \(\omega_\text{min}\) = 0.65,
            \(\omega_\text{max}\) = 6.5. (Well, more precisely, they used frequencies
            from 0.1 to 1.0 when working with integer row/column positions ranging from
            0 to 13, but here I'm expressing the frequencies w.r.t positions normalized
            to [-1.0, 1.0].)
        </p>

        <h4>CIFAR10 ViT(dim=384, mlp_dim=768, depth=6) / patch 4 @ 200 epochs</h4>
        <div class="xscroll">
            <table>
                <tr>
                    <th>Method</th>
                    <th>Learned</th>
                    <th>\(\omega_\text{min}\)</th>
                    <th>\(\omega_\text{max}\)</th>
                    <th>Valid NLL (â)</th>
                    <th>Valid Accuracy (%) (â)</th>
                </tr>
                <tr>
                    <td style="text-align: left">APE</td>
                    <td class="bigcol">â</td>
                    <td>N/A</td>
                    <td>N/A</td>
                    <td>0.4287 Â± 0.0031</td>
                    <td>89.70 Â± 0.03</td>
                </tr>
                <tr>
                    <td style="text-align: left">SinCos</td>
                    <td class="bigcol"></td>
                    <td>1.00</td>
                    <td>100.0</td>
                    <td>0.4144 Â± 0.0124</td>
                    <td>89.93 Â± 0.45</td>
                </tr>
                <tr>
                    <td style="text-align: left">Axial RoPE</td>
                    <td class="bigcol"></td>
                    <td>0.50</td>
                    <td>50.0</td>
                    <td>0.3535 Â± 0.0018</td>
                    <td>91.95 Â± 0.05</td>
                </tr>
                <!-- <tr>
                <td></td>
                <td class="bigcol">â</td>
                <td>1.0</td>
                <td>100.0</td>
                <td>0.3694 Â± 0.0009</td>
                <td>91.37 Â± 0.05</td>
            </tr> -->
                <tr>
                    <td style="text-align: left">Mixed RoPE, original freqs</td>
                    <td class="bigcol">â</td>
                    <td>0.65</td>
                    <td>6.5</td>
                    <td>0.3550 Â± 0.0072</td>
                    <td>91.63 Â± 0.32</td>
                </tr>
                <tr>
                    <td style="text-align: left">Mixed RoPE, adjusted freqs</td>
                    <td class="bigcol">â</td>
                    <td>1.00</td>
                    <td>100.0</td>
                    <td>0.3394 Â± 0.0015</td>
                    <td>92.43 Â± 0.07</td>
                </tr>
                <tr>
                    <td style="text-align: left">LieRE</td>
                    <td class="bigcol">â</td>
                    <td>N/A</td>
                    <td>N/A</td>
                    <td>0.3461 Â± 0.0023</td>
                    <td>91.95 Â± 0.04</td>
                </tr>
                <!-- <tr>
                <td>Uniform RoPE, random init</td>
                <td class="bigcol">â</td>
                <td>1.0</td>
                <td>100.0</td>
                <td>0.3382 Â± 0.0005</td>
                <td>92.25 Â± 0.01</td>
            </tr> -->
                <tr>
                    <td style="text-align: left">Golden gate RoPE</td>
                    <td class="bigcol"></td>
                    <td>1.00</td>
                    <td>100.0</td>
                    <td>0.3292 Â± 0.0023</td>
                    <td>92.43 Â± 0.05</td>
                </tr>
            </table>
        </div>

        <p>
            Both absolute position embedding methods performed poorly. Mixed RoPE
            underperformed when using the frequencies from the official implementation,
            but after adjusting the frequencies, mixed RoPE outperformed axial RoPE and
            did about as well as golden gate RoPE. During hyperparam tuning, axial RoPE was
            the only approach which benefited from a frequency range of (0.5-50) rather
            than (1.0-100.0) on CIFAR10. LieRE performed slightly better than axial
            RoPE when LieRE's skew symmetric parameters were not shared between layers
            or heads, though this resulted in high memory usage in my implementation
            compared to the other approaches, and still underperformed compared to mixed
            or golden gate RoPE. Golden gate RoPE had the best NLL.
        </p>

        <h3>ImageNet-1K</h3>

        <p>
            I trained ViT B/16 sized models (86M parameters) on ImageNet-1K for 90
            epochs. I broadly used the same data augmentation and preprocessing scheme
            as <a href="https://arxiv.org/abs/2205.01580">Beyer et al. 2022</a>,
            training at 224x224 with inception cropping and a small amount of
            <a href="https://arxiv.org/abs/1909.13719">RandAugment</a> and <a
                href="https://arxiv.org/abs/1710.09412">MixUp</a>, though with the same
            architecture and optimizer setup as the CIFAR10 experiments above.

            Due to resource limits, I only compared the fixed sinusoidal positional
            embedding, axial RoPE, mixed RoPE, and golden gate RoPE.

            See hyperparameters <a href="#imagenet1k_hyperparams">here</a> and code <a
                href="https://github.com/jxiong21029/archibox/blob/main/archibox/ropend/imagenet_classification.py">here</a>.
        </p>

        <h4>ImageNet-1K ViT B/16 @ 90 epochs</h4>
        <div class="xscroll">
            <table>
                <tr>
                    <th>Method</th>
                    <th>Learned</th>
                    <th>\(\omega_\text{min}\)</th>
                    <th>\(\omega_\text{max}\)</th>
                    <th>Zero freqs</th>
                    <th>Valid NLL (â)</th>
                    <th>Valid Accuracy (%) (â)</th>
                </tr>
                <tr>
                    <td style="text-align: left">SinCos</td>
                    <td class="bigcol"></td>
                    <td>1.0</td>
                    <td>100.0</td>
                    <td>N/A</td>
                    <td>0.8444</td>
                    <td>78.71</td>
                </tr>
                <tr>
                    <td style="text-align: left">Axial RoPE</td>
                    <td class="bigcol"></td>
                    <td>0.2</td>
                    <td>20.0</td>
                    <td>0 / 16</td>
                    <td>0.8034</td>
                    <td>79.58</td>
                </tr>
                <tr>
                    <td style="text-align: left">"</td>
                    <td class="bigcol"></td>
                    <td>"</td>
                    <td>"</td>
                    <td>8 / 16</td>
                    <td>0.8055</td>
                    <td>79.61</td>
                </tr>
                <tr>
                    <td style="text-align: left">Mixed RoPE</td>
                    <td class="bigcol">â</td>
                    <td>0.2</td>
                    <td>20.0</td>
                    <td>N/A</td>
                    <td>0.8025</td>
                    <td>79.73</td>
                </tr>
                <tr>
                    <td style="text-align: left">Golden gate RoPE</td>
                    <td class="bigcol"></td>
                    <td>0.2</td>
                    <td>20.0</td>
                    <td>0 / 32</td>
                    <td>0.8064</td>
                    <td>79.67</td>
                </tr>
                <tr>
                    <td style="text-align: left">"</td>
                    <td class="bigcol"></td>
                    <td>"</td>
                    <td>"</td>
                    <td>8 / 32</td>
                    <td>0.7979</td>
                    <td>79.78</td>
                </tr>
                <tr>
                    <td style="text-align: left">"</td>
                    <td class="bigcol"></td>
                    <td>"</td>
                    <td>"</td>
                    <td>16 / 32</td>
                    <td>0.8002</td>
                    <td>79.68</td>
                </tr>
            </table>
        </div>

        <p>
            I searched over \(\omega_\text{min} \in \{0.2, 0.5, 1.0\}\), and, in
            contrast to CIFAR10, I found that lower frequency magnitudes (0.2-20.0)
            performed better for the RoPE approaches on ImageNet. In my initial runs,
            axial RoPE, mixed RoPE, and golden gate RoPE performed about the same, with
            mixed RoPE at a slight advantage.

            From analyzing the learned frequencies of mixed RoPE, I found that a good
            portion of the learned frequencies decreased to almost zero during training.
            Based on <a href="https://github.com/KellerJordan/modded-nanogpt">modded-nanogpt</a> and
            <a href="https://arxiv.org/abs/2410.06205">Barbero et al. 2024</a>,
            I set 8 / 32 of the frequencies of golden gate RoPE to zero and it
            outperformed mixed RoPE by a slim margin. (Here, 8 / 32 means that 8 of the
            32 unique frequency magnitudes were set to zero; axial RoPE repeats the
            frequency magnitudes for x and y, so there were only 16 unique frequency
            magnitudes total.)
        </p>

        <p>
            Here, I evaluated how well each method generalized to different resolutions
            at inference time. The models were trained at 224x224 only, and to evaluate
            at a higher resolution (e.g. 384x384), the positions of each patch were
            scaled to still span [-1.0, 1.0] (so adjacent patches end up with
            coordinates which are closer together than before). I also tried scaling the
            temperature of the softmax to account for the increased token count (like
            e.g. <a href="https://arxiv.org/abs/2501.19399">here</a>), using
            a temperature of
            \(\log(\text{new_res}^2 / p^2)\) \(/ \log (\text{old_res}^2 / p^2)\)
            in this case, where \(p\) is the patch size.
        </p>

        <h4>ViT B/16 resolution generalization: validation accuracies (%) (vs in-dist @ 224x224)</h4>
        <div class="xscroll">
            <table>
                <tr>
                    <th>Method</th>
                    <th>Learned</th>
                    <th>\(\omega_\text{min}\)</th>
                    <th>\(\omega_\text{max}\)</th>
                    <th>Zero freqs</th>
                    <th>224x224 (in-dist)</th>
                    <th>384x384</th>
                    <th>384x384 w/ temp</th>
                    <th>512x512 w/ temp</th>
                </tr>
                <tr>
                    <td>SinCos</td>
                    <td class="bigcol"></td>
                    <td>1.0</td>
                    <td>100.0</td>
                    <td>N/A</td>
                    <td>78.71</td>
                    <td>75.12 (-3.59)</td>
                    <td>77.29 (-1.42)</td>
                    <td>74.95 (-3.76)</td>
                </tr>
                <tr>
                    <td>Axial RoPE</td>
                    <td class="bigcol"></td>
                    <td>0.2</td>
                    <td>20.0</td>
                    <td>0 / 16</td>
                    <td>79.58</td>
                    <td>78.31 (-1.27)</td>
                    <td>79.57 (-0.01)</td>
                    <td>77.18 (-2.40)</td>
                </tr>
                <tr>
                    <td>Mixed RoPE</td>
                    <td class="bigcol">â</td>
                    <td>0.2</td>
                    <td>20.0</td>
                    <td>N/A</td>
                    <td>79.73</td>
                    <td>78.61 (-1.12)</td>
                    <td>79.83 (+0.10)</td>
                    <td>77.55 (-2.18)</td>
                </tr>
                <tr>
                    <td>Golden gate RoPE</td>
                    <td class="bigcol"></td>
                    <td>0.2</td>
                    <td>20.0</td>
                    <td>8 / 32</td>
                    <td>79.78</td>
                    <td>79.19 (-0.59)</td>
                    <td>80.41 (+0.63)</td>
                    <td>79.15 (-0.63)</td>
                </tr>
            </table>
        </div>

        <p>
            Axial RoPE generalized better than the fixed sinuisoidal positional
            embedding, and mixed RoPE generalized better than axial RoPE. Surprisingly,
            golden gate RoPE generalized better than mixed RoPE, and both mixed RoPE and
            golden gate RoPE actually had higher validation accuracy at 384x384 when
            combined with temperature scaling than they had at the training resolution
            of 224x224.
        </p>

        <p>
            As a sidenote, I found that thinner vision transformers (dim=384,
            mlp_dim=768, 15M params) with smaller 8x8 patches and the same depth=12 (due
            to increased # of patches, approximately equivalent in FLOPs to B/16)
            resulted in improved performance across the board:
        </p>

        <h4>ImageNet-1K ViT(dim=384, mlp_dim=768, depth=12) / patch 8 @ 90 epochs</h4>
        <div class="xscroll">
            <table>
                <tr>
                    <th>Method</th>
                    <th>Learned</th>
                    <th>\(\omega_\text{min}\)</th>
                    <th>\(\omega_\text{max}\)</th>
                    <th>Valid NLL (â)</th>
                    <th>Valid Accuracy (%) (â)</th>
                </tr>
                <tr>
                    <td style="text-align: left">SinCos</td>
                    <td class="bigcol"></td>
                    <td>1.0</td>
                    <td>100.0</td>
                    <td>0.7834</td>
                    <td>79.46</td>
                </tr>
                <tr>
                    <td style="text-align: left">Axial RoPE</td>
                    <td class="bigcol"></td>
                    <td>0.2</td>
                    <td>20.0</td>
                    <td>0.7393</td>
                    <td>80.48</td>
                </tr>
                <tr>
                    <td style="text-align: left">Mixed RoPE</td>
                    <td class="bigcol">â</td>
                    <td>0.2</td>
                    <td>20.0</td>
                    <td>0.7455</td>
                    <td>80.26</td>
                </tr>
                <tr>
                    <td style="text-align: left">Golden gate RoPE</td>
                    <td class="bigcol"></td>
                    <td>0.2</td>
                    <td>20.0</td>
                    <td>0.7456</td>
                    <td>80.42</td>
                </tr>
            </table>
        </div>
        <p>
            (These runs used nonzero initialization frequencies only, though I'd expect
            better performance with some frequencies set to zero.)
        </p>

        <h2>Discussion</h2>
        <p>
            Overall, it seems like golden gate RoPE was consistently among the best
            approaches when the frequency magnitudes were properly tuned. Mixed RoPE,
            with learnable frequencies, also performed well. However, mixed RoPE,
            despite being learnable, was still sensitive to the initialization
            magnitudes of the frequencies, and demonstrated poorer generalization to
            different resolutions. I would recommend defaulting to golden gate RoPE, and
            performing at least a small amount of tuning on the RoPE frequency
            magnitudes regardless of the approach used.
        </p>

        <h2>Reference implementations</h2>
        <h3>Golden gate RoPE, 2d (PyTorch)</h3>

        <figure>
            <pre class="code"><code class="language-python">class GoldenGateRoPE2d(nn.Module):
    def __init__(
        self,
        image_size: tuple[int, int],
        n_heads: int,
        head_dim: int,
        min_freq: float,
        max_freq: float,
        p_zero_freqs: float = 0.0,
        direction_spacing: float = math.pi * (math.sqrt(5) - 1) / 2,
    ):
        """
        Args:
            image_size: expected height and width of (patchified) input
            n_heads: number of attention heads
            head_dim: attention head dimensionality
            min_freq, max_freq: lowest and highest nonzero frequency magnitudes
            p_zero_freqs: proportion of frequencies set to 0
            direction_spacing: difference in radians between adjacent directions along
                which position is measured
        
        Dimension key:
            N: batch size
            H: image_size[0]
            W: image_size[1]
            h: n_heads
            d: head_dim
            F: num_freqs == d // 2
        """
        super().__init__()
        assert head_dim % 2 == 0
        assert 0 <= p_zero_freqs <= 1
        n_freqs = head_dim // 2
        n_zero_freqs = round(p_zero_freqs * n_freqs)
        omega_F = torch.cat(
            (
                torch.zeros(n_zero_freqs),
                min_freq
                * (max_freq / min_freq) ** torch.linspace(0, 1, n_freqs - n_zero_freqs),
            )
        )
        phi_hF = (
            torch.arange(n_heads * n_freqs).reshape(n_heads, n_freqs)
            * direction_spacing
        )
        directions_hF2 = torch.stack((torch.cos(phi_hF), torch.sin(phi_hF)), dim=-1)
        freqs_hF2 = omega_F.unsqueeze(-1) * directions_hF2

        H, W = image_size
        xlim, ylim = math.sqrt(W / H), math.sqrt(H / W)
        x_HW = torch.linspace(-xlim, xlim, W).reshape(1, W).expand(H, W)
        y_HW = torch.linspace(-ylim, ylim, H).reshape(H, 1).expand(H, W)
        positions_HW112 = torch.stack((x_HW, y_HW), dim=-1).reshape(H, W, 1, 1, 2)

        theta_HWhF = (freqs_hF2 * positions_HW112).sum(dim=-1)
        self.register_buffer("cos_HWhF", torch.cos(theta_HWhF))
        self.register_buffer("sin_HWhF", torch.sin(theta_HWhF))

    def forward(self, input_NHWhd: torch.Tensor) -> torch.Tensor:
        x_NHWhF, y_NHWhF = input_NHWhd.float().chunk(2, dim=-1)
        x_out_NHWhF = x_NHWhF * self.cos_HWhF - y_NHWhF * self.sin_HWhF
        y_out_NHWhF = x_NHWhF * self.sin_HWhF + y_NHWhF * self.cos_HWhF
        output_NHWhd = torch.cat((x_out_NHWhF, y_out_NHWhF), dim=-1)
        return output_NHWhd.type_as(input_NHWhd)</code></pre>
            <div class="codecaption">
                This implementation assumes a fixed input height/width and precomputes
                the rotation matrix. <br><br>

                <code>direction_spacing</code> has a default value of \(\pi / \varphi
                \approx 1.9416\) here, which should be the best performing value, but
                note that the experiments above mistakenly used a slightly suboptimal
                value of \(2\pi / \varphi \approx 3.8832\) instead. <br><br>

                <code>direction_spacing</code> can be set to \(\pi / 2\) to
                (approximately) emulate axial RoPE.
            </div>
        </figure>

        <h3 id="ropend_impl">Golden Gate RoPE, Nd (PyTorch)</h3>

        <figure>
            <pre class="code"><code class="language-python">def _phi(m: int) -> float:
    x = 2.0
    for _ in range(10):
        x = (1 + x) ** (1.0 / (m + 1.0))
    return x


def make_directions(n: int, d: int) -> torch.Tensor:
    g = _phi(d)
    alpha = (1.0 / g) ** torch.arange(1, d + 1, dtype=torch.float64)
    i = torch.arange(1, n + 1, dtype=torch.float64).unsqueeze(1)
    z = torch.fmod(i * alpha, 1.0)
    directions = torch.erfinv(2.0 * z - 1.0)
    directions = directions / directions.norm(dim=1, keepdim=True)
    return directions.float()


class GoldenGateRoPENd(nn.Module):
    def __init__(
        self,
        pos_dim: int,
        n_heads: int,
        head_dim: int,
        min_freq: float,
        max_freq: float,
        p_zero_freqs: float = 0.0,
    ):
        """
        Args:
            pos_dim: dimensionality of the token positions
            n_heads: number of attention heads
            head_dim: attention head dimensionality
            min_freq, max_freq: lowest and highest nonzero frequency magnitudes
            p_zero_freqs: proportion of frequencies set to 0

        Dimension key:
            N: batch size
            L: number of tokens per sample
            P: pos_dim
            h: n_heads
            d: head_dim
            F: num_freqs == head_dim // 2
        """
        super().__init__()
        n_freqs = head_dim // 2
        n_zero_freqs = round(p_zero_freqs * n_freqs)
        omega_F = torch.cat(
            (
                torch.zeros(n_zero_freqs),
                min_freq
                * (max_freq / min_freq) ** torch.linspace(0, 1, n_freqs - n_zero_freqs),
            )
        )

        directions_hFP = make_directions(n_heads * n_freqs, pos_dim).reshape(
            n_heads, n_freqs, pos_dim
        )
        self.register_buffer("freqs_hFP", directions_hFP * omega_F.reshape(n_freqs, 1))

    def forward(self, input_NLhd: torch.Tensor, pos_NLP: torch.Tensor) -> torch.Tensor:
        x_NLhF, y_NLhF = input_NLhd.float().chunk(2, dim=-1)
        theta_NLhF = (self.freqs_hFP * pos_NLP[..., None, None, :].float()).sum(dim=-1)
        cos_NLhF = torch.cos(theta_NLhF)
        sin_NLhF = torch.sin(theta_NLhF)
        x_out_NLhF = x_NLhF * cos_NLhF - y_NLhF * sin_NLhF
        y_out_NLhF = x_NLhF * sin_NLhF + y_NLhF * cos_NLhF
        output_NLhd = torch.cat((x_out_NLhF, y_out_NLhF), dim=-1)
        return output_NLhd.type_as(input_NLhd)</code></pre>
            <div class="codecaption">
                This implementation takes the token positions as an additional input to
                forward().
            </div>
        </figure>

        <h2>Hyperparameters</h2>

        <h3 id="cifar10_hyperparams">CIFAR10</h3>
        <ul>
            <li>
                Augmentations: ColorJitter(brightness=0.1, contrast=0.1, saturation=0.05,
                hue=0.025), random padding by up to 4 pixels, and a random horizontal flip
                with p=0.5
            </li>
            <li>
                Architecture: RMSNorm on the input to each residual block, the queries, and
                the keys; ReLU^2 activations; learnable zero-init per-channel scaling on the
                last linear layer of each block; and no biases.
            </li>
            <li>
                Optimizers: Muon for linear layers. AdamW for per-channel scalings, and,
                when applicable, the learnable APE, the frequency vectors for mixed
                RoPE, or the raw parameters for LieRE. The schedule was constant
                learning rate with no warmup, followed by linear cooldown to 0.
            </li>

            <li>
                For the learnable APE baseline, I additionally tuned the initialization std
                and found that a relatively large value of 0.5 worked best, though learnable
                APE still performed the worst on CIFAR10 out of all tested positional
                embedding schemes.
            </li>
        </ul>
        <table>
            <tr>
                <td>Steps</td>
                <td>10,000 (200 epochs)</td>
            </tr>
            <tr>
                <td>Batch size</td>
                <td>1,000</td>
            </tr>
            <tr>
                <td>Muon LR</td>
                <td>0.06</td>
            </tr>
            <tr>
                <td>Muon momentum</td>
                <td>0.95</td>
            </tr>
            <tr>
                <td>Muon weight decay</td>
                <td>0.01</td>
            </tr>
            <tr>
                <td>AdamW LR</td>
                <td>0.003</td>
            </tr>
            <tr>
                <td>AdamW betas</td>
                <td>(0.9, 0.95)</td>
            </tr>
            <tr>
                <td>AdamW weight decay</td>
                <td>0.01</td>
            </tr>
            <tr>
                <td>LR cooldown start</td>
                <td>7,500</td>
            </tr>
            <tr>
                <td>Label smoothing</td>
                <td>0.1</td>
            </tr>
            <tr>
                <td>Patch size</td>
                <td>4</td>
            </tr>
            <tr>
                <td>dim</td>
                <td>384</td>
            </tr>
            <tr>
                <td>MLP dim</td>
                <td>768</td>
            </tr>
            <tr>
                <td>depth</td>
                <td>6</td>
            </tr>
        </table>

        <h3 id="imagenet1k_hyperparams">ImageNet-1K</h3>
        <ul>
            <li>Augmentations: RandAugment(2, 10) and MixUp(alpha=0.2)</li>
            <li>Everything else same as the CIFAR10 experiments unless otherwise specified</li>
        </ul>

        <table>
            <tr>
                <td>Steps</td>
                <td>187,650 (90 epochs)</td>
            </tr>
            <tr>
                <td>Global batch size</td>
                <td>1,024</td>
            </tr>
            <tr>
                <td>Muon LR</td>
                <td>0.03</td>
            </tr>
            <tr>
                <td>Muon momentum</td>
                <td>0.95</td>
            </tr>
            <tr>
                <td>Muon weight decay</td>
                <td>0.01</td>
            </tr>
            <tr>
                <td>AdamW LR</td>
                <td>0.001</td>
            </tr>
            <tr>
                <td>AdamW betas</td>
                <td>(0.9, 0.95)</td>
            </tr>
            <tr>
                <td>AdamW weight decay</td>
                <td>0.01</td>
            </tr>
            <tr>
                <td>LR cooldown start</td>
                <td>150,120 (70 epochs)</td>
            </tr>
            <tr>
                <td>Patch size</td>
                <td>16</td>
            </tr>
            <tr>
                <td>dim</td>
                <td>768</td>
            </tr>
            <tr>
                <td>MLP dim</td>
                <td>3072</td>
            </tr>
            <tr>
                <td>depth</td>
                <td>12</td>
            </tr>
        </table>

        <h2>Acknowledgements</h2>
        <p>
            <a href="https://github.com/ad8e">Kevin Yin</a> for suggesting fixed
            rotations based on the golden ratio, as well as general feedback.
            <a href="https://cgdct.moe/">Stephen Huan</a> for general feedback.
        </p>

        <h2>How to cite</h2>
        <div class="figure">
            <pre class="code"><code>@misc{xiong2025ndrope,
    author = {Jerry Xiong},
    title = {On n-dimensional rotary positional embeddings},
    year = {2025},
    url = {https://jerryxio.ng/posts/nd-rope/}
}</code></pre>
        </div>

        <ol style="color: gray">
            <li id="footnote_1">
                In an earlier version of this post, the suggested approach (selecting
                frequency directions rotated based on the golden ratio) was called
                uniform RoPE. Fluffy on the EleutherAI discord suggested the name
                'golden gate RoPE'. <a href="#footnote_1_ref">â</a>
            </li>
        </ol>
    </div>

    <script type="module" src="rope_fig1.js"></script>
    <script type="module" src="rope_fig1_axial.js"></script>
    <script type="module" src="rope_fig2.js"></script>
    <script type="module" src="rope_fig3.js"></script>
    <script>hljs.highlightAll();</script>
</body>

</html>